# -*- coding: utf-8 -*-
"""CNN_Predict_E_coli_Resistance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WcVqvbkXXMGvT6RcwYzmLvO0TebtXkN-

# Title: CNN model to predict antibiotic resistance of different strains

Build convolutional neural network (CNN) to predicte of AMR for the antibiotics ciprofloxacin (CIP), cefotaxime (CTX), ceftazidime (CTZ), and gentamicin (GEN).

  * Label encoding: The A, G, C, T, N in the SNP_matrix were converted to 1, 2, 3, 4, and 0.

**Estefanos Kebebew**

#### Download the package for the png
"""

#!pip install git+https://gitlab.com/drj11/pypng@pypng-0.20220715.0

"""## Import packges"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import matplotlib as mpl
import time
import os
import io
import png
import seaborn as sns
import glob
import pickle
from PIL import Image
import random
import cv2
import png
import numpy as np
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
from pathlib import Path
from sklearn.model_selection import train_test_split
import torch
from sklearn.metrics import confusion_matrix
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.models import Model
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler
import keras
import keras.layers as layers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from sklearn.model_selection import train_test_split

"""## Read the file"""

from google.colab import drive
drive.mount('/content/drive')

giessen_orig = pd.read_csv("https://raw.githubusercontent.com/Estefanos8080/snps-to-2dcnn-converter/main/Giessen_Dataset/cip_ctx_ctz_gen_multi_data.csv")

giessen_orig.head()

"""### The Drug column will contain the list of drugs, and the Resistance column will contain the list of resistance values. The resistance values will be encoded as 1 for resistance and 0 for susceptibility."""

giessen = pd.read_csv("https://raw.githubusercontent.com/Estefanos8080/snps-to-2dcnn-converter/main/Giessen_Dataset/cip_ctx_ctz_gen_pheno.csv")
giessen.head()

"""### merge the dataframes"""

# merge the two dataframe
df = pd.merge(giessen, giessen_orig, on='prename', how='inner')
df.head()

"""### Keep only the desired features"""

selected_features = ['prename', 'CIP', 'CTX', 'CTZ', 'GEN']
df_selected = df[selected_features]

df_selected.tail()

# get the shape of the dataframe
df.shape

"""# Change each isolates into image

In the dataset:
  * A = 1, G = 2 , C = 3 , T = 4  and N = 0.
  * When assign a pixel value A(1) is Red, (2)is yellow, C(3)is Green, T(4)is  blue, G and Null(1) = white

"""

# drop prename, CIP, CTX, CTZ and GEN
df.drop(columns=['prename', 'CIP', 'CTX', 'CTZ', 'GEN'], inplace=True)

df.tail()

# shape of the newly created dataframe
print("New dataframe shape: ", df.shape)

"""#### Create a dictionary that maps the labels to RGB values. The labels are the numbers 0, 1, 2, 3, and 4. The RGB values are the colors red, green, blue, and yellow."""

# Create a dictionary that maps the labels to RGB values.
label_to_rgb = {
    0: (255, 255, 255),
    1: (255, 0, 0),
    2: (255, 255, 0),
    3: (0, 255, 0),
    4: (0, 0, 255),
}

"""### image_height = total number of features / image_width = 60936 / 312 â‰ˆ 195.  
* I experimented with different values and found that the maximum width that can fit within the given data shape is 312.
"""

image_width = 312
image_height = 195  # Total number of features divided by image_width

"""* The block of code below create an image with the dimension defined above where the determined by a color mapping stored in the label to rgb dictionary. The resulting list of image captures genetic variation in visual form."""

images = []
for i in range(df.shape[0]):
    sequence = df.iloc[i].values
    image = np.zeros((image_height, image_width, 3), dtype=np.uint8)
    for j in range(image_height * image_width):
        rgb_value = label_to_rgb[sequence[j]]
        x = j % image_width
        y = j // image_width
        image[y, x] = rgb_value
    images.append(image)

len(images)

"""### Display one image from the list."""

for n in range(len(images)):
  plt.imshow(images[n])
  plt.show()
  break

""" Create a directory to save the images if it doesn't exist"""

image_dir = 'image_representation_of_each_sample'
if not os.path.exists(image_dir):
  os.makedirs(image_dir)

"""### Save each original image in the directory that is created above"""

# Save each image
for i, image in enumerate(images):
    plt.imshow(image)
    plt.savefig(os.path.join(image_dir, f'image_{i}.png'))  # Save each isolate image with a unique filename in the directory
    plt.close()

"""### Instead of pulling the images from the saved file, Let's create different list that will be used to merge with the selected features (drugs and isolate name)"""

# Create a list that is similar to the saved image
final_images = []
for i in range(len(images)):
    final_images.append(images[i])

# Create a DataFrame with the images
df_images = pd.DataFrame({'image': final_images})
df_images.head()

"""### create the final DataFrame that will be used to train the model"""

# merge the selected features(the AR's and the isolate name) with the image
final_df = pd.concat([df_selected, df_images], axis=1)
final_df.head()

# To save the DataFrame to a CSV file:
# final_df.to_csv('prename_CIP_CTX_CTZ_GEN_3channels2Dimages_dataset.csv', index=False)

"""### Create a DataFrame that contains each drug with its isolate name and the corresponding 3D channel image"""

# build a model to predcit the resistance of CIP
def create_df_for_each_AMR(drug_name):
  CIP_df = final_df[['prename',drug_name, 'image']]
  return CIP_df
CIP_df = create_df_for_each_AMR('CIP')
CTZ_df = create_df_for_each_AMR('CTZ')
CTX_df = create_df_for_each_AMR('CTX')
GEN_df = create_df_for_each_AMR('GEN')


CIP_df.head()

# display the image and wheter it is resisnce or not where 1 stands for resistance
def display_each_sample_image(new_crated_df):
  for n in range(len(new_crated_df)):
    resistance = ''
    # example lets get the 700th element
    CIP_value = new_crated_df.iloc[600][1]
    if CIP_value == 1:
      resistance = 'resistance'
    else:
      resistance = 'susceptibility'
    plt.title(f'{new_crated_df.iloc[600][0]} {resistance}')
    plt.axis('off')
    plt.imshow(new_crated_df.iloc[600][2])
    plt.show()
    break

display_each_sample_image(CIP_df)

# get the shape of the image
print("Image shape: ", CIP_df.iloc[0][2].shape)


def process_df(d_f, drug):
    images = np.array(d_f["image"].tolist())
    cip_resistance = np.array(d_f[drug].tolist())
    return images, cip_resistance

images, cip_resistance = process_df(CIP_df, 'CIP')

# print(len(cip_resistance))

"""The 'train_ratio' variable is set to 0.8, indicating that 80% of the data will be used for training, and the remaining 20% will be used for testing. The 'split_index' is calculated by multiplying the 'train_ratio' with the total number of samples in the 'CIP_df' DataFrame and converting it to an integer. The 'images' and 'cip_resistance' arrays are then split into 'train_images' and 'test_images', and 'train_labels' and 'test_labels', respectively, using the 'split_index'."""

# Split the data into training and testing sets
# train_ratio = 0.8  # 80% of the data for training
# split_index = int(train_ratio * len(CIP_df))
# train_images, test_images = images[:split_index], images[split_index:]
# train_labels, test_labels = cip_resistance[:split_index], cip_resistance[split_index:]

def split_data(d_f,resistance):
  train_ratio = 0.8  # 80% of the data for training
  split_index = int(train_ratio * len(d_f))
  train_images, test_images = images[:split_index], images[split_index:]
  train_labels, test_labels = resistance[:split_index], resistance[split_index:]
  return train_images, train_labels, test_images, test_labels

train_images, train_labels, test_images, test_labels = split_data(CIP_df, cip_resistance)


print(len(train_images), len(train_labels), len(test_images), len(test_labels))

"""##### The function takes two parameters, 'images' and 'labels', and scales the pixel values from the original range of [0, 255] to the normalized range of [0, 1]."""

def Normalize_data(images, labels):
    # Normalize the images by scaling the pixel values to [0, 1]
    normalized_images = images.astype('float32') / 255.0

    # Label encoding for binary classification (0 or 1)
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(labels)

    # Convert the labels to one-hot encoded format
    one_hot_labels = to_categorical(encoded_labels)

    return normalized_images, one_hot_labels

"""  * As mentioned above: the Normalize_data function normalizes the pixel values of the input images, scaling them to the range [0, 1]. It also performs label encoding to convert the binary labels (0 or 1) into numerical representations. The training data, after normalization and encoding, is now stored in 'train_images_normalized' and 'train_labels_encoded', while the testing data is stored in 'test_images_normalized' and 'test_labels_encoded'"""

# Preprocess the data
train_images_normalized, train_labels_encoded = Normalize_data(train_images, train_labels)
test_images_normalized, test_labels_encoded = Normalize_data(test_images, test_labels)

# Create a tuple containing the normalized images and one-hot encoded labels
train_data = (train_images_normalized, train_labels_encoded)
val_data = (test_images_normalized, test_labels_encoded)

"""# CNN model

* The model architecture includes six blocks of convolutions, each followed by batch normalization to stabilize training and rectified linear unit (ReLU) activation for introducing non-linearity.
* Max pooling is applied after each convolutional block to downsample the spatial dimensions. To improve generalization and prevent overfitting, dropout layers are introduced after each max pooling layer, with varying dropout rates.
* The fully connected layers at the end of the model have more neurons, enabling the model to learn higher-level representations.
"""

model = keras.Sequential([
    layers.Conv2D(64, kernel_size=(3, 3), activation="relu", input_shape=(195, 312, 3)),
    layers.MaxPooling2D(pool_size=(2, 2)),

    # Block 1
    layers.Conv2D(128, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.Conv2D(128, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.MaxPooling2D(pool_size=(2, 2), padding='same'),


    # Block 2
    layers.Conv2D(128, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(128, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2), padding='same'),
    layers.Dropout(0.3),

    # Block 3
    layers.Conv2D(512, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.Conv2D(512, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.MaxPooling2D(pool_size=(2, 2), padding='same'),


    # Block 4
    layers.Conv2D(512, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.Conv2D(512, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.MaxPooling2D(pool_size=(2, 2), padding='same'),


    # Block 5
    layers.Conv2D(512, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(512, kernel_size=(3, 3), activation="relu", padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2), padding='same'),
    layers.Dropout(0.5),

    # Fully Connected Layers
    layers.Flatten(),
    layers.Dense(256, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(2, activation="softmax")
])

# model.summary()

"""Print out the model artecture how each layer was concatente one to another."""

import tensorflow as tf
import pydot
from IPython.display import Image

def custom_plot_model_with_color(model):
    # Convert the model to a dot format
    dot_model = tf.keras.utils.model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir='TB')

    # Modify the appearance of the nodes and edges in the graph
    for node in dot_model.get_node_list():
        if node.get_label():
            label = node.get_label().replace('\"', '')
            color = '#E57373'  # Red color for Convolutional layers
            if 'MaxPooling2D' in label:
                color = '#64B5F6'  # Blue color for MaxPooling layers
            elif 'Dense' in label:
                color = '#81C784'  # Green color for Dense layers
            node.set_fillcolor(color)
            node.set_style('filled')

    for edge in dot_model.get_edge_list():
        edge.set_color('#424242')  # Set the edge color to dark gray

    # Generate the image and save it as a PNG file
    graph = pydot.graph_from_dot_data(dot_model.to_string())[0]
    graph.write_png('custom_model_with_color.png')

    # Display the image
    return Image('custom_model_with_color.png')

custom_plot_model_with_color(model)

"""Evaluate trhe metrics using accuracy, optmiizer adam, and loss function was sparse categrorical"""

# Compile the model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

"""Train the model"""

# Train the model using the preprocessed data with integer labels
history = model.fit(train_images_normalized, train_labels, validation_data=(test_images_normalized, test_labels), epochs=20, batch_size=30)

"""Get the learning curve"""

# Plot the learning curve
def plot_learning_curve(history):
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['loss'], label='Training Loss')
    plt.title('Learning Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy / Loss')
    plt.legend(loc='upper right')
    plt.show()

plot_learning_curve(history)

"""Create a function to make predictions on the test dataset"""

# predicted_labels = model.predict(test_images_normalized)
def return_predicted_labels(noramlized_image_test):
  predicted_labels = model.predict(noramlized_image_test)
  return predicted_labels

return_predicted_labels(test_images_normalized)

# Randomly select three indices from the test dataset for visualization
np.random.seed(42)  # Set a seed for reproducibility
random_indices = np.random.choice(len(test_images_normalized), 3, replace=False)

import numpy as np
import matplotlib.pyplot as plt

def visualize_random_images_with_predictions(test_images_normalized, test_labels, model):
    np.random.seed(42)  # Set a seed for reproducibility
    random_indices = np.random.choice(len(test_images_normalized), 3, replace=False)
    selected_images = test_images_normalized[random_indices]

    predicted_labels = model.predict(selected_images)

    plt.figure(figsize=(12, 8))
    for i, index in enumerate(random_indices, 1):
        plt.subplot(3, 2, i*2-1)
        plt.imshow(test_images[index])
        plt.title(f"Actual: {test_labels[index]}")
        plt.axis('off')

        plt.subplot(3, 2, i*2)
        plt.bar(range(2), predicted_labels[i - 1], tick_label=["0", "1"])
        plt.title(f"Predicted: {np.argmax(predicted_labels[i - 1])}")
        plt.xlabel("CIP")
        plt.ylabel("Probability")
        plt.xticks(fontsize=10)

    plt.tight_layout()
    plt.show()


visualize_random_images_with_predictions(test_images_normalized, test_labels, model)

# Make predictions on the test dataset
predicted_labels = model.predict(test_images_normalized)
predicted_classes = np.argmax(predicted_labels, axis=1)

predicted = np.argmax(predicted_labels, axis=1)

"""####The correct_predictions variable counts the number of correctly predicted samples, and total_samples represents the total number of test samples. The accuracy is then calculated by dividing the number of correct predictions by the total number of samples"""

correct_predictions = np.sum(predicted_classes)
total_samples = len(test_labels_encoded)
accuracy = (correct_predictions / total_samples) * 100

print(f"Accuracy on the test set: {accuracy:.2f}")

"""Get the true classes from the test labels"""

true_classes = test_labels

# Create the confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)

"""Plot the confusion matrix as a heatmap"""

plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')

# Annotate the heatmap with TP, TN, FP, and FN values
for i in range(len(cm)):
    for j in range(len(cm)):
        if i == j:
            plt.text(j + 0.2, i + 0.2, f"TP={cm[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')
        else:
            plt.text(j + 0.2, i + 0.2, f"TN={cm[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""uncomment the code below to save the trained model"""

#save the model
# model.save('cip_model.h5')

"""## Train the model for cefotaxime (CTX)"""

CTX_df.head()

images, CTX_resistance = process_df(CTX_df, 'CTX')

train_images_CTX, train_labels_CTX, test_images_CTX, test_labels_CTX = split_data(CTX_df,CTX_resistance)
print(len(train_images_CTX), len(train_labels_CTX), len(test_images_CTX), len(test_labels_CTX))

# Preprocess the data
train_images_normalized_CTX, train_labels_encoded_CTX = Normalize_data(train_images_CTX, train_labels_CTX)
test_images_normalized_CTX, test_labels_encoded_CTX = Normalize_data(test_images_CTX, test_labels_CTX)

# Create a tuple containing the normalized images and one-hot encoded labels
train_data_CTX = (train_images_normalized_CTX, train_labels_encoded_CTX)
val_data_CTX = (test_images_normalized_CTX, test_labels_encoded_CTX)

history2 = model.fit(train_images_normalized_CTX, train_labels_CTX, validation_data=(test_images_normalized_CTX, test_labels_CTX), epochs=20, batch_size=30)

plot_learning_curve(history2)

visualize_random_images_with_predictions(test_images_normalized_CTX, test_labels_CTX, model)

predicted_labels_CTX = model.predict(test_images_normalized_CTX)
predicted_classes_CTX = np.argmax(predicted_labels_CTX, axis=1)

predicted_CTX = np.argmax(predicted_labels_CTX, axis=1)

correct_predictions_CTX = np.sum(predicted_classes_CTX)
total_samples_CTX = len(test_labels_encoded_CTX)
accuracy_CTX = (correct_predictions_CTX / total_samples_CTX)

print(f"Accuracy on the test set: {accuracy_CTX:.6f}")

true_classes_CTX = test_labels_CTX

# Create the confusion matrix
cm_CTX = confusion_matrix(true_classes_CTX, predicted_classes_CTX)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 8))
sns.heatmap(cm_CTX, annot=True, fmt='d', cmap='Blues')

# Annotate the heatmap with TP, TN, FP, and FN values
for i in range(len(cm_CTX)):
    for j in range(len(cm_CTX)):
        if i == j:
            plt.text(j + 0.2, i + 0.2, f"TP={cm_CTX[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')
        else:
            plt.text(j + 0.2, i + 0.2, f"TN={cm_CTX[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""## Train the model for ceftazidime (CTZ)"""

CTZ_df.head()

"""The process df function thay was created above tooks the dataset and drug as a parameter then create a new dataframe which has two elemets one is the converted image of the sample and there reponse to the drug."""

images, CTZ_resistance = process_df(CTZ_df, 'CTZ')

train_images_CTZ, train_labels_CTZ, test_images_CTZ, test_labels_CTZ = split_data(CTZ_df, CTZ_resistance)

# Preprocess the data
train_images_normalized_CTZ, train_labels_encoded_CTZ = Normalize_data(train_images_CTZ, train_labels_CTZ)
test_images_normalized_CTZ, test_labels_encoded_CTZ = Normalize_data(test_images_CTZ, test_labels_CTZ)

# Create a tuple containing the normalized images and one-hot encoded labels
train_data_CTZ = (train_images_normalized_CTZ, train_labels_encoded_CTZ)
val_data_CTZ = (test_images_normalized_CTZ, test_labels_encoded_CTZ)

history3  = model.fit(train_images_normalized_CTZ, train_labels_CTZ, validation_data=(test_images_normalized_CTZ, test_labels_CTZ), epochs=20, batch_size=30)

plot_learning_curve(history3)

visualize_random_images_with_predictions(test_images_normalized_CTZ, test_labels_CTZ, model)

predicted_labels_CTZ = model.predict(test_images_normalized_CTZ)
predicted_classes_CTZ = np.argmax(predicted_labels_CTZ, axis=1)

predicted_CTZ = np.argmax(predicted_labels_CTZ, axis=1)

correct_predictions_CTZ = np.sum(predicted_classes_CTZ)
total_samples_CTZ = len(test_labels_encoded_CTZ)
accuracy_CTZ = (correct_predictions_CTZ / total_samples_CTZ)

print(f"Accuracy on the test set: {accuracy_CTZ:.6f}")

true_classes_CTZ = test_labels_CTZ

# Create the confusion matrix
cm_CTZ = confusion_matrix(true_classes_CTZ, predicted_classes_CTX)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 8))
sns.heatmap(cm_CTZ, annot=True, fmt='d', cmap='Blues')

# Annotate the heatmap with TP, TN, FP, and FN values
for i in range(len(cm_CTZ)):
    for j in range(len(cm_CTZ)):
        if i == j:
            plt.text(j + 0.2, i + 0.2, f"TP={cm_CTZ[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')
        else:
            plt.text(j + 0.2, i + 0.2, f"TN={cm_CTZ[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""##Train the model for gentamicin (GEN)"""

GEN_df.head()

images, GEN_resistance = process_df(GEN_df, 'GEN')

train_images_GEN, train_labels_GEN, test_images_GEN, test_labels_GEN = split_data(GEN_df, GEN_resistance)

# Preprocess the data
train_images_normalized_GEN, train_labels_encoded_GEN = Normalize_data(train_images_GEN, train_labels_GEN)
test_images_normalized_GEN, test_labels_encoded_GEN = Normalize_data(test_images_GEN, test_labels_GEN)

# Create a tuple containing the normalized images and one-hot encoded labels
train_data_GEN = (train_images_normalized_GEN, train_labels_encoded_GEN)
val_data_GEN = (test_images_normalized_GEN, test_labels_encoded_GEN)

history4 = model.fit(train_images_normalized_GEN, train_labels_GEN, validation_data=(test_images_normalized_GEN, test_labels_GEN), epochs=20, batch_size=30)

plot_learning_curve(history4)

visualize_random_images_with_predictions(test_images_normalized_GEN, test_labels_GEN, model)

predicted_labels_GEN = model.predict(test_images_normalized_GEN)
predicted_classes_GEN = np.argmax(predicted_labels_GEN, axis=1)

predicted_GEN = np.argmax(predicted_labels_GEN, axis=1)

correct_predictions_GEN = np.sum(predicted_classes_GEN)
total_samples_GEN = len(test_labels_encoded_GEN)
accuracy_GEN = (correct_predictions_GEN / total_samples_GEN) * 100

print(f"Accuracy on the test set: {accuracy_GEN:.2f}")

true_classes_GEN = test_labels_GEN

# Create the confusion matrix
cm_GEN = confusion_matrix(true_classes_GEN, predicted_classes_GEN)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 8))
sns.heatmap(cm_GEN, annot=True, fmt='d', cmap='Blues')

# Annotate the heatmap with TP, TN, FP, and FN values
for i in range(len(cm_GEN)):
    for j in range(len(cm_GEN)):
        if i == j:
            plt.text(j + 0.2, i + 0.2, f"TP={cm_GEN[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')
        else:
            plt.text(j + 0.2, i + 0.2, f"TN={cm_GEN[i, j]}", ha='center', va='center', color='black', fontsize=12, fontweight='bold')

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
